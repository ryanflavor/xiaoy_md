# Story 3.3: Market Data Monitoring Dashboards
**故事 3.3：行情监控与告警仪表盘**

## Status
Ready for Review

## Story
**As a** DevOps Engineer,
**I want** dashboards and alerts for core feed metrics,
**so that** we can detect coverage gaps, latency spikes, or rate-limit incidents in real time.

## Acceptance Criteria
1. Prometheus scrape targets or exporters defined for the market-data-service and subscription scripts.
2. Grafana dashboard template captures subscription coverage, messages-per-second, latency percentiles, error counts, and rate-limit triggers.
3. Alert thresholds and notification channels documented for sustained anomalies.

## Tasks / Subtasks
- [x] Implement Prometheus exporters and scrape configuration (AC: 1) [Source: docs/architecture/observability-and-ops.md#telemetry-architecture]
  - [x] Wire `prometheus-client` metrics exporters into the market-data-service so `md_throughput_mps`, `md_latency_ms_p99`, and `md_error_count` are exposed on a dedicated metrics port. [Source: docs/architecture/observability-and-ops.md#metric-catalog]
  - [x] Extend subscription workers (`scripts/operations/full_feed_subscription.py`) to expose `md_subscription_coverage_ratio` and `md_rate_limit_hits` via an HTTP exporter or Pushgateway bridge, following the CLI flag pattern documented for metrics ports. [Source: docs/architecture/observability-and-ops.md#telemetry-architecture]
  - [x] Add scrape jobs under `config/prometheus/prometheus.yml` (or equivalent) for both services, ensuring sample intervals (5s/60s) match the architecture guidance and runbook expectations. [Source: docs/ops/monitoring-dashboard.md#4-dashboard-provisioning-配置]
  - [x] Update deployment assets (docker-compose profiles or runbook instructions) so Prometheus and Pushgateway endpoints are reachable during `start_live_env.sh` orchestration. [Source: docs/architecture/infrastructure-and-deployment.md#monitoring--alerting-integration]
- [x] Deliver Grafana dashboard assets (AC: 2) [Source: docs/ops/monitoring-dashboard.md#2-grafana-dashboard-layout-仪表盘布局]
  - [x] Produce `docs/ops/templates/grafana/md_ops_dashboard.json` covering executive overview, latency/errors, operations, and drilldown rows with panels for all required metrics. [Source: docs/architecture/observability-and-ops.md#dashboards]
  - [x] Document dashboard version, provisioning steps, and expected panels in `docs/ops/monitoring-dashboard.md`, referencing the new JSON template for traceability. [Source: docs/ops/monitoring-dashboard.md#4-dashboard-provisioning-配置]
  - [x] Ensure dashboard annotations ingest runbook events (`md_runbook_exit_code`, `md_failover_latency_ms`) so operations can correlate alerts with orchestration actions. [Source: docs/architecture/observability-and-ops.md#integration-with-runbooks]
- [x] Define alert thresholds and notification channels (AC: 3) [Source: docs/architecture/observability-and-ops.md#alerting-model]
  - [x] Update the alert catalog in `docs/ops/monitoring-dashboard.md` with Slack/PagerDuty routing details, escalation paths, and runbook links for each condition. [Source: docs/ops/monitoring-dashboard.md#3-alert-rules-告警规则]
  - [x] Capture notification wiring (Slack webhook, PagerDuty service, on-call rotation) in the PRD NFR section or appendices so stakeholder expectations stay visible. [Source: docs/prd/requirements-要求.md#non-functional-requirements-nfr]
  - [x] Provide validation steps (e.g., synthetic alert test script or `promtool test rules`) to confirm alerts fire and resolve, wiring them into the runbook checklist. [Source: docs/ops/production-runbook.md#5-routine-health-checks-日常健康巡检]
- [x] Add verification and regression coverage (AC: 1, 2, 3) [Source: docs/architecture/test-strategy-and-standards.md#testing-philosophy]
  - [x] Implement unit or integration tests that exercise the metrics exporters (e.g., hitting `/metrics` endpoints) and ensure required time-series are present with expected labels. [Source: docs/architecture/observability-and-ops.md#telemetry-architecture]
  - [x] Add automated checks ensuring the Grafana JSON template remains valid (e.g., lint via `grafana-dashboard-validator` or schema test) and that alert definitions include notification channels. [Source: docs/ops/monitoring-dashboard.md#5-validation-checklist-验证清单]
  - [x] Extend CI or local scripts to fail fast when metrics endpoints are unavailable or alert templates regress, aligning with the observability quality gate. [Source: docs/architecture/infrastructure-and-deployment.md#monitoring--alerting-integration]

## Dev Notes

### Previous Story Insights
- Story 3.1 established Pushgateway metrics from `start_live_env.sh` (`md_runbook_exit_code`, `md_failover_latency_ms`), so dashboards must surface these automation signals alongside service metrics. [Source: docs/stories/3.1.story.md#component-specifications]
- Story 3.2 formalized `.env` governance, including monitoring toggles and Pushgateway endpoints, so dashboard provisioning should reference those configuration keys instead of introducing new ones. [Source: docs/stories/3.2.story.md#technical-constraints]
- Story 2.4.4 delivered structured `mps_report` logs and counters but left Prometheus exporter support optional, making this story responsible for finalizing exporter exposure and scrape targets. [Source: docs/stories/2.4.4.story.md#tasks--subtasks]
- Story 2.5 documented throughput evidence and soak procedures that the dashboards need to visualize for ongoing verification. [Source: docs/stories/2.5.story.md#dev-notes]

### Data Models
- Metrics naming and labels follow the Telemetry Architecture table (`md_throughput_mps`, `md_subscription_coverage_ratio`, etc.); exporters should register gauges/counters matching those identifiers. [Source: docs/architecture/observability-and-ops.md#metric-catalog]
- Alert payloads should include tags for session window (`day|night`) and feed (`primary|backup`) to align with failover analysis. [Source: docs/ops/monitoring-dashboard.md#1-metrics-inventory-指标清单]

### API Specifications
- No specific API contract changes; telemetry endpoints must follow Prometheus exposition format over HTTP and integrate with Pushgateway where direct scraping is impractical. [Source: docs/architecture/observability-and-ops.md#telemetry-architecture]

### Component Specifications
- Observability workflows govern Prometheus integration and Grafana provisioning; reuse the documented dashboard lifecycle for packaging templates under version control. [Source: docs/architecture/observability-and-ops.md#dashboards]
- Dashboard automation must hook into runbook telemetry loops to annotate failovers and restarts for post-incident review. [Source: docs/architecture/observability-and-ops.md#integration-with-runbooks]
- Subscription worker now runs as the `subscription-worker` compose service during runbook orchestration so Prometheus can scrape `subscription-worker:9101`; any future tooling must preserve this container entrypoint. [Source: docker-compose.yml#L134]
- `verify_consumer_metrics` supports `DRILL_EXPECT_FEED` / `DRILL_EXPECT_ACCOUNT` selectors so drills focus on the active feed; keep these environment knobs aligned with any future feed-label changes. [Source: scripts/operations/start_live_env.sh#L184]

### File Locations
- Prometheus configuration and templates belong under `config/prometheus/` per the architecture guide; create the directory if absent. [Source: docs/architecture/observability-and-ops.md#dependencies--configuration]
- Grafana JSON templates reside in `docs/ops/templates/grafana/`, retaining version updates in git for drift detection. [Source: docs/ops/monitoring-dashboard.md#4-dashboard-provisioning-配置]
- Service instrumentation changes stay within `src/application/` and subscription tooling under `scripts/operations/`. [Source: docs/architecture/source-tree.md#10-source-tree]

### Testing Requirements
- Use pytest to validate exporter availability, simulate alert conditions, and assert that dashboards/alerts include required metrics and notification wiring. [Source: docs/architecture/test-strategy-and-standards.md#testing-philosophy]
- Provide smoke tests or fixtures to confirm Pushgateway submissions appear in Prometheus during orchestration dry runs. [Source: docs/ops/production-runbook.md#1-pre-market-startup-盘前启动]

### Technical Constraints
- Maintain JSON logging and Asia/Shanghai timestamp policy when instrumentation logs events alongside metrics. [Source: docs/architecture/coding-standards.md#critical-rules]
- Do not hard-code secrets in dashboard templates; rely on `.env` keys and Vault guidance established in Story 3.2. [Source: docs/stories/3.2.story.md#technical-constraints]
- Ensure Prometheus scrape configuration respects environment segmentation (dev/test/live) by referencing `.env` profiles documented in the runbook. [Source: docs/ops/production-runbook.md#1-pre-market-startup-盘前启动]

## Project Structure Notes
- Follow the defined source tree when introducing monitoring assets: code under `src/`, operational scripts under `scripts/operations/`, dashboards in `docs/ops/templates/grafana/`, and Prometheus configs in `config/prometheus/`. [Source: docs/architecture/source-tree.md#10-source-tree]

## Dev Agent Record

### Agent Model Used
GPT-5 (Codex CLI)

### Debug Log References
N/A (no debug log entries required)

### Completion Notes List
- Added Prometheus instrumentation for market-data-service with latency P99, throughput, and error counters.
- Exposed subscription coverage/rate limit metrics via CLI exporter and created Prometheus scrape definitions.
- Delivered Grafana dashboard template plus documentation updates, including alert wiring and synthetic alert smoke test script.
- Augmented unit tests and documentation checks to cover new observability assets.
- Containerized the subscription worker (`subscription-worker` service) so Prometheus can scrape metrics at `subscription-worker:9101`, resolving prior QA gap.

### File List
- config/prometheus/prometheus.yml
- docker-compose.yml
- docs/ops/templates/grafana/md_ops_dashboard.json
- docs/ops/monitoring-dashboard.md
- docs/ops/production-runbook.md
- docs/architecture/observability-and-ops.md
- docs/prd/requirements-要求.md
- src/application/observability.py
- src/application/services.py
- src/__main__.py
- src/main.py
- src/config.py
- src/operations/full_feed_subscription.py
- scripts/operations/start_live_env.sh
- scripts/operations/alert_smoke.py
- tests/unit/test_observability_metrics.py
- tests/unit/operations/test_full_feed_subscription_cli.py
- tests/test_documentation.py

## QA Results

### Review Date: 2025-09-22

### Reviewer
- Quinn (Test Architect)

### Code Quality Assessment
- Metrics exporters and dashboards are well-documented and backed by targeted tests, but the Prometheus scrape wiring does not match how the subscription worker is launched.
- Because Prometheus looks for `subscription-worker:9101` while the runbook starts the worker as a host process, the coverage and rate-limit metrics never reach Prometheus/Grafana, blocking AC 1 and AC 2.

### Compliance Check
- Coding Standards: ✓
- Project Structure: ✓
- Testing Strategy: ✓ Targeted pytest suites cover exporters, dashboard specs, and runbook orchestration
- All ACs Met: ✗ Prometheus cannot reach the subscription metrics endpoint

### Defects & Risks
- [ ] Prometheus job `subscription-workers` targets `subscription-worker:9101`, but `start_subscription_worker` runs `uv run scripts/operations/full_feed_subscription.py` on the host with no container/hostname binding. Prometheus therefore never connects and dashboards stay empty. (config/prometheus/prometheus.yml:17, scripts/operations/start_live_env.sh:237)

### Tests Executed
- `uv run pytest --no-cov tests/unit/test_observability_metrics.py`
- `uv run pytest --no-cov tests/unit/operations/test_full_feed_subscription_cli.py`
- `uv run pytest --no-cov tests/unit/operations/test_start_live_env.py`
- `uv run pytest --no-cov tests/test_documentation.py::TestMonitoringArtifacts`

### Gate Status
- Gate: FAIL → docs/qa/gates/3.3-market-data-monitoring-dashboards.yml
- Recommended Status: ✗ Changes Required
---

### Review Date: 2025-09-22 (Re-Review)

### Reviewer
- Quinn (Test Architect)

### Code Quality Assessment
- Running the subscription worker as a docker-compose service keeps the metrics exporter reachable at `subscription-worker:9101`, and the runbook now waits for the container health check before proceeding (`docker-compose.yml:134`, `scripts/operations/start_live_env.sh:277`).
- Prometheus targets and labels align with that container, so dashboards ingest coverage and rate-limit time-series once the stack comes online (`config/prometheus/prometheus.yml:17`).
- Documentation and entrypoints were updated to reflect the containerized worker, keeping operators aligned with the new orchestration path (`docs/ops/monitoring-dashboard.md:69`, `scripts/operations/full_feed_subscription.py:1`).

### Compliance Check
- Coding Standards: ✓
- Project Structure: ✓
- Testing Strategy: ✓ Targeted suites exercise exporters, CLI wiring, and runbook orchestration
- All ACs Met: ✓ Metrics jobs now resolve the subscription worker endpoint

### Defects & Risks
- [x] Prometheus Job `subscription-workers` resolves successfully because the worker runs inside compose and exposes the metrics port on the service hostname.

### Tests Executed
- `uv run pytest --no-cov tests/unit/test_observability_metrics.py`
- `uv run pytest --no-cov tests/unit/operations/test_full_feed_subscription_cli.py`
- `uv run pytest --no-cov tests/unit/operations/test_start_live_env.py`
- `uv run pytest --no-cov tests/test_documentation.py::TestMonitoringArtifacts`

### Gate Status
- Gate: PASS → docs/qa/gates/3.3-market-data-monitoring-dashboards.yml
- Recommended Status: ✓ Ready for Done


## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-22 | 0.1 | Draft story created | Bob (Scrum Master) |
| 2025-09-24 | 1.0 | Implemented observability exporters, dashboard assets, and alert validation tooling | James (Developer) |
